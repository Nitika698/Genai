# ğŸš€ LLaMA-2 Mini Pretraining on Custom Dataset

This repository contains a fully working implementation of **pretraining a miniature LLaMA-2 style model** on a custom text dataset using HuggingFace Transformers.

The project trains a downsized LLaMA architecture on your own data and then uses it for text generation.

---

## ğŸ“Œ Features

- ğŸ”¹ Train a **custom small LLaMA model** from scratch  
- ğŸ”¹ Uses **AutoTokenizer** + LLaMA architecture  
- ğŸ”¹ Works on **CPU** (slow but possible) or GPU  
- ğŸ”¹ Supports any `.txt` dataset  
- ğŸ”¹ Saves trained model + tokenizer  
- ğŸ”¹ Includes text generation pipeline  

---

## ğŸ“‚ Project Structure

generative_ai/
â”‚â”€â”€ pretraining.py # Main script
â”‚â”€â”€ first.txt # Custom dataset
â”‚â”€â”€ llama2-mini-model/ # Saved model + tokenizer
â”‚â”€â”€ README.md # Documentation

yaml
Copy code

---

## ğŸ§  Requirements

Install the required libraries:

```bash
pip install transformers datasets accelerate sentencepiece
If using LLaMA-2 (gated model), login first:

bash
Copy code
huggingface-cli login
ğŸ“ Dataset
Place your dataset at:

makefile
Copy code
C:\generative_ai\first.txt
Example content:

csharp
Copy code
Cricket is a popular sport played between two teams...
Your model will learn patterns from this file.

â–¶ï¸ Running the Training
Run:

bash
Copy code
python pretraining.py
Expected output:

vbnet
Copy code
train_loss: 9.11
epoch: 3.0
Model saved to ./llama2-mini-model
ğŸ’¾ Output Files
The trained model is saved at:

pgsql
Copy code
llama2-mini-model/
â”‚â”€â”€ config.json
â”‚â”€â”€ pytorch_model.bin
â”‚â”€â”€ tokenizer.json
â”‚â”€â”€ tokenizer_config.json
â”‚â”€â”€ special_tokens_map.json
You can load them anytime for inference.

ğŸ§ª Inference Example
python
Copy code
from transformers import pipeline

generator = pipeline(
    "text-generation",
    model="./llama2-mini-model",
    tokenizer="./llama2-mini-model"
)

prompt = "What is cricket?"
output = generator(prompt, max_length=100)

print(output[0]["generated_text"])
âš ï¸ Windows Terminal Unicode Fix
If your terminal throws:

vbnet
Copy code
UnicodeEncodeError: 'charmap' codec can't encode character
Add:

python
Copy code
import sys
sys.stdout.reconfigure(encoding='utf-8')
â­ Future Improvements
Add LoRA fine-tuning

Add GPU training support

Add multiple dataset support

Create a Chat UI around the model

ğŸ“„ License
MIT License â€” free to use and modify.

